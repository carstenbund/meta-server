# LLM Provider Configuration Example
# Copy this file to .env and configure your preferred provider(s)

# ============================================
# Provider Selection
# ============================================
# Specify which provider to use explicitly (optional)
# If not set, will auto-select based on available API keys
# Options: openai, anthropic, ollama
# LLM_PROVIDER=openai

# Provider preference order (comma-separated)
# Will try each provider in order until one succeeds
LLM_PROVIDER_PREFERENCE=openai,anthropic,ollama

# ============================================
# OpenAI Configuration
# ============================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ============================================
# Anthropic Configuration
# ============================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-haiku-20241022

# Note: Anthropic does not provide embeddings.
# For embeddings, use OpenAI or Ollama.

# ============================================
# Ollama Configuration (Local Models)
# ============================================
# Ollama runs models locally, no API key needed
# Install Ollama from: https://ollama.ai/
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ============================================
# General LLM Settings
# ============================================
LLM_TIMEOUT=60
LLM_MAX_RETRIES=3

# Enable embedding generation (requires OpenAI or Ollama)
ENABLE_EMBEDDINGS=false

# ============================================
# Index Worker Settings
# ============================================
DATABASE_URI=sqlite:///instance/files.db
NUM_WORKERS=4
POLL_INTERVAL=2
